# è®­ç»ƒè„šæœ¬ä½¿ç”¨æŒ‡å—

## ğŸ“‹ å¿«é€Ÿå¼€å§‹

### 1. å¤åˆ¶æ¨¡æ¿è„šæœ¬

```bash
cd train/
cp sft_bench_TEMPLATE.sh train_my_dataset.sh
chmod +x train_my_dataset.sh
```

### 2. ä¿®æ”¹å…³é”®å‚æ•°

æ‰“å¼€ `train_my_dataset.sh`ï¼Œä¿®æ”¹ä»¥ä¸‹æ ‡è®°ä¸º `[éœ€è¦ä¿®æ”¹]` çš„éƒ¨åˆ†ï¼š

## ğŸ”§ å¿…é¡»ä¿®æ”¹çš„å‚æ•°

### âœ… 1. å®éªŒè¾“å‡ºè·¯å¾„

```bash
# ä¿®æ”¹å®éªŒè¾“å‡ºç›®å½•
EXP_DIR='/mnt/tidal-alsh01/dataset/perceptionVLM/exps_zixu/vllm/llava_ov/'

# ä¿®æ”¹ä¿å­˜è·¯å¾„ï¼ˆå»ºè®®æ ¼å¼ï¼šstage2_sft_æ•°æ®é›†å_æ—¥æœŸï¼‰
SAVE_CKPT_PATH=$EXP_DIR/stage2_sft_vqa_$(date +%m%d)
```

**ç¤ºä¾‹å‘½å**ï¼š
- `stage2_sft_vqa_1220` - VQAæ•°æ®é›†ï¼Œ12æœˆ20æ—¥
- `stage2_sft_agriculture_1210` - å†œä¸šæ•°æ®é›†ï¼Œ12æœˆ10æ—¥
- `stage2_sft_chemistry_1202` - åŒ–å­¦æ•°æ®é›†ï¼Œ12æœˆ2æ—¥

### âœ… 2. æ•°æ®è·¯å¾„

```bash
# æŒ‡å‘ä½ çš„ WebDataset ç›®å½•
# ç›®å½•ç»“æ„åº”è¯¥æ˜¯ï¼š
#   /path/to/webdatasets/
#   â”œâ”€â”€ .nv-meta/          # Megatron ç´¢å¼•ç›®å½•
#   â”œâ”€â”€ subtaskdata-00000.tar
#   â”œâ”€â”€ subtaskdata-00001.tar
#   â””â”€â”€ ...
DATA_PATH=/mnt/tidal-alsh01/dataset/perceptionVLMData/processed_v2.0/YOUR_DATASET/webdatasets/
```

**é‡è¦**ï¼šç¡®ä¿è¿™ä¸ªè·¯å¾„ä¸‹ï¼š
1. âœ… æœ‰ `.nv-meta` ç›®å½•ï¼ˆç”± `stage2_sftdata2webdataset.py` ç”Ÿæˆï¼‰
2. âœ… æœ‰ `.tar` æ–‡ä»¶ï¼ˆWebDataset æ•°æ®æ–‡ä»¶ï¼‰

### âœ… 3. Batch Size é…ç½®

```bash
# æ ¹æ®ä½ çš„ GPU æ•°é‡å’Œæ˜¾å­˜è°ƒæ•´
MBS=1          # Micro Batch Sizeï¼ˆæ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°ï¼‰
GBS=8          # Global Batch Sizeï¼ˆå…¨å±€æ‰¹æ¬¡å¤§å°ï¼‰
```

**è®¡ç®—å…¬å¼**ï¼š
```
GBS = MBS Ã— GPUæ•°é‡
```

**ç¤ºä¾‹**ï¼š
- 4ä¸ªGPUï¼ŒMBS=1 â†’ GBS=4
- 8ä¸ªGPUï¼ŒMBS=1 â†’ GBS=8
- 8ä¸ªGPUï¼ŒMBS=2 â†’ GBS=16
- 32ä¸ªGPUï¼ŒMBS=1 â†’ GBS=32

**è€å¸ˆå»ºè®®**ï¼š`GBS = 4 Ã— GPUæ•°é‡` æˆ– `GBS = 8 Ã— GPUæ•°é‡`

### âœ… 4. è®­ç»ƒæ­¥æ•°

```bash
# è®¡ç®—å…¬å¼ï¼šNSTEP â‰ˆ æ ·æœ¬æ•°é‡ / GBS
NSTEP=3200
```

**è®¡ç®—å…¬å¼**ï¼š
```
NSTEP = æ ·æœ¬æ•°é‡ / GBS
```

**ç¤ºä¾‹**ï¼š
- æ ·æœ¬æ•°ï¼š100,000ï¼ŒGBS=32 â†’ NSTEP=3,125ï¼ˆå»ºè®®è®¾ä¸º3200ï¼‰
- æ ·æœ¬æ•°ï¼š50,000ï¼ŒGBS=16 â†’ NSTEP=3,125ï¼ˆå»ºè®®è®¾ä¸º3200ï¼‰
- æ ·æœ¬æ•°ï¼š10,000ï¼ŒGBS=8 â†’ NSTEP=1,250ï¼ˆå»ºè®®è®¾ä¸º1300ï¼‰

**å»ºè®®**ï¼šå¯ä»¥ç¨å¾®è®¾ç½®å¤šä¸€ç‚¹ï¼Œæ¯”å¦‚å¤š 5-10%ï¼Œä»¥ä¾¿å®Œæ•´è®­ç»ƒä¸€ä¸ª epoch

### âœ… 5. æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„

```bash
# é€‰æ‹©ä½ çš„èµ·å§‹æ£€æŸ¥ç‚¹
CHECKPOINT_PATH=/path/to/your/checkpoint
```

**é€‰é¡¹è¯´æ˜**ï¼š

1. **ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼‰**
   ```bash
   CHECKPOINT_PATH=/mnt/tidal-alsh01/dataset/perceptionVLM/models/LLaVA-OneVision-1.5-4B-Base
   ```

2. **ä½¿ç”¨ Stage0 æ¨¡å‹**
   ```bash
   CHECKPOINT_PATH=/mnt/tidal-alsh01/dataset/perceptionVLM/models/LLaVA-OneVision-1.5-4B-stage0
   ```

3. **ä½¿ç”¨ Stage2 æ¨¡å‹ï¼ˆæ¨èï¼Œç»§ç»­è®­ç»ƒï¼‰**
   ```bash
   CHECKPOINT_PATH=/mnt/tidal-alsh01/dataset/perceptionVLM/models/LLaVA-OneVision-1.5-4B-stage2_mcore_tp1_pp1
   ```

4. **æ–­ç‚¹ç»­è®­ï¼ˆä½¿ç”¨ä¹‹å‰è®­ç»ƒçš„æ£€æŸ¥ç‚¹ï¼‰**
   ```bash
   CHECKPOINT_PATH=/mnt/tidal-alsh01/dataset/perceptionVLM/exps_zixu/vllm/llava_ov/stage2_sft_previous/iter_0400
   ```

### âœ… 6. GPU æ•°é‡ï¼ˆå¦‚æœæ˜¯å•æœºï¼‰

```bash
# åœ¨ç¬¬ 65 è¡Œå·¦å³
GPUS_PER_NODE=2    # ä¿®æ”¹ä¸ºä½ çš„å®é™…GPUæ•°é‡ï¼ˆå•æœºè®­ç»ƒæ—¶ï¼‰
```

## ğŸ” å¯èƒ½éœ€è¦è°ƒæ•´çš„å‚æ•°

### å­¦ä¹ ç‡

```bash
--lr 1.0e-5    # é»˜è®¤å­¦ä¹ ç‡ï¼Œé€šå¸¸ä¸éœ€è¦ä¿®æ”¹
```

å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼ˆæŸå¤±çˆ†ç‚¸æˆ–ä¸‹é™å¾ˆæ…¢ï¼‰ï¼Œå¯ä»¥è°ƒæ•´ï¼š
- å­¦ä¹ ç‡å¤ªå¤§ï¼šé™ä½åˆ° `5e-6` æˆ– `1e-6`
- å­¦ä¹ ç‡å¤ªå°ï¼šå¢åŠ åˆ° `2e-5` æˆ– `5e-5`

### ä¿å­˜é—´éš”

```bash
--save-interval 200    # æ¯200æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
```

æ ¹æ®è®­ç»ƒæ­¥æ•°è°ƒæ•´ï¼š
- NSTEP=3200 â†’ save-interval=200ï¼ˆä¿å­˜16ä¸ªæ£€æŸ¥ç‚¹ï¼‰
- NSTEP=1000 â†’ save-interval=100ï¼ˆä¿å­˜10ä¸ªæ£€æŸ¥ç‚¹ï¼‰

### æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°

```bash
--num-workers 16    # æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°
```

å»ºè®®è®¾ç½®ä¸ºï¼š
- CPUæ ¸å¿ƒæ•°çš„ä¸€åŠï¼Œæˆ–
- GPUæ•°é‡çš„ 2-4 å€

## ğŸš€ è¿è¡Œè®­ç»ƒ

### 1. æ£€æŸ¥é…ç½®

åœ¨è¿è¡Œå‰ï¼Œç¡®è®¤ï¼š

```bash
# 1. æ•°æ®è·¯å¾„å­˜åœ¨ä¸”åŒ…å« .nv-meta å’Œ .tar æ–‡ä»¶
ls -la $DATA_PATH/.nv-meta/
ls -la $DATA_PATH/*.tar | head -5

# 2. æ£€æŸ¥ç‚¹è·¯å¾„å­˜åœ¨
ls -la $CHECKPOINT_PATH/

# 3. Tokenizer è·¯å¾„å­˜åœ¨
ls -la $TOKENIZER_PATH/
```

### 2. è¿è¡Œè®­ç»ƒ

```bash
bash train_my_dataset.sh
```

### 3. ç›‘æ§è®­ç»ƒ

**æŸ¥çœ‹æ—¥å¿—**ï¼š
```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f $SAVE_CKPT_PATH/run_*.log

# æŸ¥çœ‹æœ€æ–°çš„æ—¥å¿—
ls -t $SAVE_CKPT_PATH/run_*.log | head -1 | xargs tail -f
```

**æŸ¥çœ‹ TensorBoard**ï¼š
```bash
tensorboard --logdir=$TENSORBOARD_PATH
```

ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€ï¼š`http://localhost:6006`

## ğŸ“Š è®­ç»ƒè¿‡ç¨‹ç›‘æ§

### å…³é”®æŒ‡æ ‡

1. **Lossï¼ˆæŸå¤±ï¼‰**ï¼šåº”è¯¥é€æ­¥ä¸‹é™
2. **Learning Rateï¼ˆå­¦ä¹ ç‡ï¼‰**ï¼šæŒ‰ä½™å¼¦æ›²çº¿è¡°å‡
3. **GPU åˆ©ç”¨ç‡**ï¼šåº”è¯¥æ¥è¿‘ 100%

### å¸¸è§é—®é¢˜

#### âŒ OOM (Out of Memory)

**è§£å†³æ–¹æ³•**ï¼š
- å‡å° `MBS`ï¼ˆMicro Batch Sizeï¼‰
- å‡å° `SEQ_LEN`ï¼ˆåºåˆ—é•¿åº¦ï¼‰
- æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨æ˜¾å­˜

#### âŒ æ•°æ®åŠ è½½æ…¢

**è§£å†³æ–¹æ³•**ï¼š
- å¢åŠ  `--num-workers`
- æ£€æŸ¥æ•°æ®è·¯å¾„çš„ç½‘ç»œé€Ÿåº¦
- ç¡®ä¿æ•°æ®åœ¨æœ¬åœ°æˆ–é«˜é€Ÿå­˜å‚¨ä¸Š

#### âŒ Loss ä¸ä¸‹é™æˆ–çˆ†ç‚¸

**è§£å†³æ–¹æ³•**ï¼š
- é™ä½å­¦ä¹ ç‡ `--lr`
- æ£€æŸ¥æ•°æ®è´¨é‡
- æ£€æŸ¥å­¦ä¹ ç‡é¢„çƒ­è®¾ç½®

## ğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„

è®­ç»ƒå®Œæˆåï¼Œ`$SAVE_CKPT_PATH` ç›®å½•ç»“æ„ï¼š

```
stage2_sft_vqa_1220/
â”œâ”€â”€ iter_0000/          # æ£€æŸ¥ç‚¹ï¼ˆæ¯ save-interval æ­¥ä¿å­˜ä¸€æ¬¡ï¼‰
â”œâ”€â”€ iter_0200/
â”œâ”€â”€ iter_0400/
â”œâ”€â”€ ...
â”œâ”€â”€ iter_3200/
â”œâ”€â”€ dataloader/         # æ•°æ®åŠ è½½å™¨ç¼“å­˜
â”œâ”€â”€ tensorboard/        # TensorBoard æ—¥å¿—
â””â”€â”€ run_2024-12-20_10:30:45_*.log  # è®­ç»ƒæ—¥å¿—
```

## ğŸ”„ æ–­ç‚¹ç»­è®­

å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œå¯ä»¥ä»æœ€è¿‘çš„æ£€æŸ¥ç‚¹ç»§ç»­ï¼š

```bash
# 1. æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹
ls -d $SAVE_CKPT_PATH/iter_* | sort -V | tail -1

# 2. ä¿®æ”¹è„šæœ¬ä¸­çš„ CHECKPOINT_PATH
CHECKPOINT_PATH=/path/to/latest/checkpoint/iter_0400

# 3. é‡æ–°è¿è¡Œè„šæœ¬
bash train_my_dataset.sh
```

## ğŸ“ å®Œæ•´ç¤ºä¾‹

å‡è®¾ï¼š
- æ•°æ®é›†ï¼šVQAï¼Œ100,000 ä¸ªæ ·æœ¬
- GPUï¼š8 ä¸ª
- æ•°æ®è·¯å¾„ï¼š`/data/vqa/webdatasets/`

```bash
# 1. è®¡ç®—å‚æ•°
GBS = 8 Ã— 4 = 32  (ä½¿ç”¨ 4Ã—GPUæ•°é‡)
NSTEP = 100000 / 32 â‰ˆ 3125 â†’ è®¾ä¸º 3200

# 2. ä¿®æ”¹è„šæœ¬
EXP_DIR='/mnt/tidal-alsh01/dataset/perceptionVLM/exps_zixu/vllm/llava_ov/'
SAVE_CKPT_PATH=$EXP_DIR/stage2_sft_vqa_1220
DATA_PATH=/data/vqa/webdatasets/
GBS=32
NSTEP=3200
GPUS_PER_NODE=8

# 3. è¿è¡Œ
bash train_my_dataset.sh
```

## ğŸ†˜ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯ä¿¡æ¯
2. ç¡®è®¤æ‰€æœ‰è·¯å¾„éƒ½å­˜åœ¨
3. ç¡®è®¤ GPU å¯ç”¨ï¼š`nvidia-smi`
4. ç¡®è®¤æ•°æ®æ ¼å¼æ­£ç¡®ï¼šä½¿ç”¨ `inspect_webdataset.py` æ£€æŸ¥

